{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EtiPeretz/Weighted-Random-Forest-implementation/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stYit-uGbayX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import *\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from  sklearn.base import BaseEstimator\n",
        "import random\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from random import randrange\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szJKiFQ-4lz1",
        "outputId": "9af00eb1-2597-484e-ff5b-761285b4ee7c"
      },
      "source": [
        "!git clone https://github.com/fe1493/Fashion_DataSet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Fashion_DataSet' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnh3LsHi1WoQ"
      },
      "source": [
        "# Section 1: Weighted Random Forest implementation\n",
        "\n",
        "In the first part, you are requested to implement a variation of Random Forest, which we will call \"weighted\" random forest (WRF vs. RF).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7NoZGUq_dw9"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pandas import read_csv\n",
        "import numpy as np\n",
        "from sklearn.tree import *\n",
        "from sklearn.metrics import mean_squared_error, zero_one_loss\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import itertools as it\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "\n",
        "\n",
        "class WRF(BaseEstimator):\n",
        "    # init a WRF classifier with the following parameters:\n",
        "    def __init__(self, n_trees=100, max_depth=5, n_features=None, type=\"cat\", weight_type=\"div\"):\n",
        "        '''\n",
        "          init a WRF classifier with the following parameters:\n",
        "          n_trees: the number of trees to use.\n",
        "          max_depth: the depth of each tree (will be passed along to DecisionTreeClassifier/DecisionTreeRegressor).\n",
        "          n_features: the number of features to use for every split. The number should be given to DecisionTreeClassifier/Regressor as max_features.\n",
        "          type: \"cat\" for categorization and \"reg\" for regression.\n",
        "          weight_type: the tree weighting technique. 'div' for 1/error and 'sub' for 1-error.\n",
        "        '''\n",
        "        # n_trees: the number of trees to use\n",
        "        self.n_trees = n_trees\n",
        "        # max_depth: the depth of each tree\n",
        "        self.max_depth = max_depth\n",
        "        # n_features: the number of features to use for every split.\n",
        "        self.n_features = n_features\n",
        "        self.type = type\n",
        "        # weight_type: the tree weighting technique.\n",
        "        self.weight_type = weight_type\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "          fit the classifier for the data X with response y.\n",
        "        '''\n",
        "        self.trees = []\n",
        "        self.weights = []\n",
        "        # <Your Code if needed>\n",
        "        for n in range(self.n_trees):\n",
        "            tree = self.build_tree()\n",
        "            self.trees.append(tree)\n",
        "            X_tree, y_tree, X_oob, y_oob = self.bootstrap(X, y)\n",
        "            tree.fit(X_tree, y_tree)\n",
        "            weight = self.calculate_weight(tree, X_oob, y_oob)\n",
        "            self.weights.append(weight)\n",
        "\n",
        "        # Normalize the weights so they sum to 1\n",
        "        temp = np.array(self.weights - min(self.weights))\n",
        "        self.weights = temp / sum(temp)  # Sum total to 1.0\n",
        "        #print(\"sum of weight is: \", sum(self.weights))\n",
        "        #self.weights = normalize(self.weights)\n",
        "\n",
        "    def build_tree(self):\n",
        "        tree = None\n",
        "        if self.type == \"cat\":\n",
        "            tree = DecisionTreeClassifier(max_depth=self.max_depth, max_features=self.n_features)\n",
        "        else:\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth, max_features=self.n_features)\n",
        "        return tree\n",
        "\n",
        "    def bootstrap(self, X, y):\n",
        "        '''\n",
        "          This method creates a bootstrap of the dataset (uniformly sample len(X) samples from X with repetitions).\n",
        "          It returns X_tree, y_tree, X_oob, y_oob.\n",
        "          X_tree, y_tree are the bootstrap collections for the given X and y.\n",
        "          X_oob, y_oob are the out of bag remaining instances (the ones that were not sampled as part of the bootstrap)\n",
        "        # '''\n",
        "        X_tree = []\n",
        "        y_tree = []\n",
        "        X_oob = []\n",
        "        y_oob = []\n",
        "        used_indexes = []\n",
        "        for i in range(len(X)):\n",
        "            n = random.randint(0, len(X) - 1)\n",
        "            used_indexes.append(n)\n",
        "            X_tree.append(X[n])\n",
        "            y_tree.append(y[n])\n",
        "        # list of all the indexes used\n",
        "        used_indexes = list(dict.fromkeys(used_indexes))\n",
        "        for i in range(len(X)):\n",
        "            # out of bag observations\n",
        "            if not (i in used_indexes):\n",
        "                X_oob.append(X[i])\n",
        "                y_oob.append(y[i])\n",
        "        return X_tree, y_tree, X_oob, y_oob\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_weight(self, tree, X_oob, y_oob):\n",
        "        '''\n",
        "          This method calculates a weight for the given tree, based on it's performance on\n",
        "          the OOB instances. We support two different types:\n",
        "          if self.weight_type == 'div', we should return 1/error and if it's 'sub' we should\n",
        "          return 1-error. The error is the normalized error rate of the tree on OOB instances.\n",
        "          For classification use 0/1 loss error (i.e., count 1 for every wrong OOB instance and divide by the numbner of OOB instances),\n",
        "          and for regression use mean square error of the OOB instances.\n",
        "        '''\n",
        "        # calculate the error\n",
        "        # For classification use 0/1 loss error\n",
        "        if self.type == \"cat\":\n",
        "            # count 1 for every wrong OOB instance and divide by the numbner of OOB instances)\n",
        "            error = zero_one_loss(tree.predict(X_oob), y_oob)\n",
        "        # for regression use mean square error of the OOB instances\n",
        "        else:\n",
        "            error = mean_squared_error(tree.predict(X_oob), y_oob)\n",
        "\n",
        "        # calculate weights\n",
        "        # if self.weight_type == 'div', we should return 1/error\n",
        "        if self.weight_type == 'div':\n",
        "            return 1 / error\n",
        "        # if it's 'sub' we should return 1-error.\n",
        "        if self.weight_type == 'sub':\n",
        "            return 1 - error\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "          Predict the label/value of the given instances in the X matrix.\n",
        "          For classification you should implement weighted voting, and for regression implement weighted mean.\n",
        "          Return a list of predictions for the given instances.\n",
        "        '''\n",
        "        predictions = []\n",
        "        pred_of_y = []\n",
        "        # For classification you should implement weighted voting\n",
        "        if self.type == \"cat\":\n",
        "            for tree in self.trees:  # prediction of single tree\n",
        "                pred_of_y.append(tree.predict(X))\n",
        "            # count the votes of preditions from trees\n",
        "            pred_of_y = np.asarray(pred_of_y)\n",
        "            for i in range(pred_of_y.shape[1]):\n",
        "                count = np.bincount(pred_of_y[:, i], weights=self.weights)\n",
        "                prediction = np.argmax(count)\n",
        "                predictions.append(prediction)\n",
        "        # for regression implement weighted mean\n",
        "        else:\n",
        "            for tree in self.trees:  # prediction of single tree\n",
        "                pred_of_y.append(tree.predict(X))\n",
        "            # multiply the prediction in the weight of trees\n",
        "            pred_of_y = np.asarray(pred_of_y)\n",
        "            #pred_of_y =  np.linalg.norm(pred_of_y)\n",
        "            self.weights = self.weights.reshape((self.n_trees, 1))\n",
        "            predictions = np.multiply(pred_of_y, self.weights)\n",
        "            # calculate the mean for prediction from all trees\n",
        "            predictions = np.mean(pred_of_y, axis=0)\n",
        "        return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AweBS3ty84vI"
      },
      "source": [
        "# Section 2: Evaluation\n",
        "\n",
        "In this section you are requested to evaluate your implementation, and compare it with RandomForestClassifier and RandomForestRegressor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt0j2Sj5LkTA"
      },
      "source": [
        "# TODO 3: Implement a tuning method for your classifier. \n",
        "# Note: you could potentially implement WRF as a sklearn classifier and then \n",
        "# simply use GridSearchCV inside. For those of you who want to take this route, \n",
        "# you are welcome to modify the implementation of WRF accordingly. Check out: https://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator\n",
        "import itertools as it\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tune(classifier, X, y, arguments, cv=5):\n",
        "  '''\n",
        "    This method is doing exactly what GridSearchCV is doing for a sklearn classifier.\n",
        "    It will run cross validation training with cv folds many times. Each time it will evaluate the CV \"performance\" on a different\n",
        "    combination of the given arguments. You should check every combination of the given arguments and return a dictionary with \n",
        "    the best argument combination. For classification, \"performance\" is accuracy. For Regression, \"performance\" is mean square error.\n",
        "    \n",
        "    classifier: it's the WRF classifier to tune\n",
        "    X, y: the dataset to tune over\n",
        "    arguments: a dictionary with keys are one of n_trees, max_depth, n_features, weight_type\n",
        "    and the values are lists of values to test for each argument (see more in GridSearchCV)\n",
        "  '''\n",
        "  \n",
        "  # <Your code goes here>\n",
        "  if classifier.type == \"cat\":\n",
        "    score = \"accuracy\"\n",
        "  else: \n",
        "      score = \"neg_mean_squared_error\"\n",
        "  gcv = GridSearchCV(classifier, arguments,score ,cv)\n",
        "  gcv.fit(X, y)\n",
        "  return gcv.best_params_\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nuDmPrXNjeg"
      },
      "source": [
        "# TODO 4: Evaluate your implementation and compare it to RandomForestClassifier/Regressor provided by sklearn.\n",
        "\n",
        "# For classification use the Fashion MNIST, but subsample the dataset to contain only 7K instances (out of the 60K available instances, you may simply select the first 7K instance from the data).\n",
        "# - Tune both classifiers (WRF and RandomForestClassifier) before evaluation.\n",
        "# - Evaluate both classifiers on the first 5K instances from the provided test data.\n",
        "# - Report accuracy, and provide a full confusion matrix for each classifier.\n",
        "\n",
        "# For regression use the California housing dataset from Kaggle that we used in class (available also in the Git repo of the course):\n",
        "# https://www.kaggle.com/harrywang/housing#housing.csv\n",
        "\n",
        "# - Split the dataset to train and test (test_size=0.1, random_state=0)\n",
        "# - Tune both regressors (WRF and RandomForestRegressor) before evaluation on the training set.\n",
        "# - Evaluate both regressors on the test set.\n",
        "# - Report mean square error.\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2A9tJCdtys"
      },
      "source": [
        "Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj8MBVywdIvY",
        "outputId": "27be7a2a-add7-459e-d142-e1a0dc120e7f"
      },
      "source": [
        "clsf_data = pd.read_csv('Fashion_DataSet/small_fashion_mnist.csv')\n",
        "clsf_test_data = pd.read_csv('Fashion_DataSet/fashion-mnist_test.csv')\n",
        "\n",
        "\n",
        "y = clsf_data.iloc[:, 1].to_numpy()\n",
        "y_test = (clsf_test_data.iloc[:5000, 0]).to_numpy()\n",
        "X = clsf_data.iloc[:, 2:].to_numpy()\n",
        "X_test = clsf_test_data.iloc[:5000, 1:].to_numpy()\n",
        "\n",
        "print(X.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "print(y.shape)\n",
        "# print(y_test.head())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 784)\n",
            "(5000, 784)\n",
            "(5000,)\n",
            "(7000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQRbccWYeKVb"
      },
      "source": [
        "wrf = WRF(type='cat')\n",
        "rf = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13swvlW9CS1v",
        "outputId": "a2596a9f-c452-40f8-b986-5ef09b093ff2"
      },
      "source": [
        "#Random Forest \n",
        "gcv = GridSearchCV(rf, {\"max_features\": [1, 2],\n",
        "                        \"n_estimators\": [5, 7, 9], \"max_depth\": [3, 4, 5]},scoring=\"accuracy\", cv=5, )\n",
        "gcv.fit(X, y)\n",
        "rf_best_params = gcv.best_params_\n",
        "print(\"RF best params are:\")\n",
        "print(rf_best_params)\n",
        "\n",
        "# #Weighted Random Forest \n",
        "wrf_arguments = {\"n_trees\": [5, 7, 9],\n",
        "                \"max_depth\": [3, 4, 5], \"n_features\": [1, 2], \"weight_type\": ['duv', 'sub']}\n",
        "# wrf_arguments = {\"n_trees\": [5],\n",
        "#                  \"max_depth\": [3], \"n_features\": [1], \"weight_type\": ['duv', 'sub']}\n",
        "wrf_best_params = tune(wrf, X, y, wrf_arguments, cv=5)\n",
        "print(\"\\nWRF best params are:\")\n",
        "print(wrf_best_params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF best params are:\n",
            "{'max_depth': 5, 'max_features': 2, 'n_estimators': 9}\n",
            "\n",
            "WRF best params are:\n",
            "{'max_depth': 5, 'n_features': 2, 'n_trees': 9, 'weight_type': 'sub'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL-yfvwG3Wjb",
        "outputId": "274c5a95-c5c1-4617-be73-99762cb6bd4f"
      },
      "source": [
        "#Weighted Random Forest best params to test\n",
        "wrf_bestparams_ready = WRF(n_trees=wrf_best_params['n_trees'], max_depth=wrf_best_params['max_depth'],\n",
        "                n_features=wrf_best_params['n_features'],type='cat', weight_type=wrf_best_params['weight_type'])\n",
        "wrf_bestparams_ready.fit(X, y)\n",
        "\n",
        "#Random Forest best params to test\n",
        "rf_bestparams_ready = RandomForestClassifier(n_estimators=rf_best_params['n_estimators'],\n",
        "                                  max_depth=rf_best_params['max_depth'],\n",
        "                                  max_features=rf_best_params['max_features'])\n",
        "rf_bestparams_ready.fit(X, y)\n",
        "\n",
        "#Predict the models\n",
        "wrf_prediction = wrf_bestparams_ready.predict(X_test)\n",
        "rf_prediction = rf_bestparams_ready.predict(X_test)\n",
        "print(\"Classification\\n\")\n",
        "print(\"WRF accuracy: \", accuracy_score(y_test, wrf_prediction))\n",
        "print(\"\\nRF accuracy: \", accuracy_score(y_test, rf_prediction))\n",
        "print(\"Confusion matrix for WRF:\\n\")\n",
        "print(confusion_matrix(y_test, wrf_prediction))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Confusion matrix for RF:\\n\")\n",
        "print(confusion_matrix(y_test, rf_prediction,))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification\n",
            "\n",
            "WRF accuracy:  0.626\n",
            "\n",
            "RF accuracy:  0.7082\n",
            "Confusion matrix for WRF:\n",
            "\n",
            "[[324   5  20 105   1   0  39   1   6   1]\n",
            " [  2 451  18  18   0   0   0   1   0   0]\n",
            " [ 11   3 428  14   2   5  22   0   4   1]\n",
            " [ 19  78  12 387   0   5   4   0   2   0]\n",
            " [  2   6 384  91   5   2  10   1   5   0]\n",
            " [  0   5  14   2   0 284   0 117   7  61]\n",
            " [ 91  12 228 104   0   0  75   1   8   0]\n",
            " [  0   0   0   0   0  13   0 415   3  40]\n",
            " [  2   8  43  40   2  17  32  12 346   6]\n",
            " [  1   2   7   6   0   7   0  56  23 415]]\n",
            "\n",
            "\n",
            "Confusion matrix for RF:\n",
            "\n",
            "[[377  12  24  55   4   2  12   0  16   0]\n",
            " [  3 447  10  28   1   0   1   0   0   0]\n",
            " [ 15   0 362   6  64   1  28   0  14   0]\n",
            " [ 43  72   9 367   8   1   6   0   1   0]\n",
            " [ 17   7 185  80 161   0  51   0   5   0]\n",
            " [  0   0   0   0   0 353   0  88   4  45]\n",
            " [133   7  97  42  56   0 165   0  19   0]\n",
            " [  0   0   0   0   0  24   0 410   0  37]\n",
            " [  1   2  12  12   1   3  12   5 454   6]\n",
            " [  1   0   0   0   0   8   0  62   1 445]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODzyeMsUn9sv"
      },
      "source": [
        "Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nInOPgcMoD7J"
      },
      "source": [
        "#Load regression data\n",
        "regress_data = pd.read_csv('Fashion_DataSet/housing.csv')\n",
        "y = regress_data[\"median_house_value\"]\n",
        "X = regress_data.drop(\"median_house_value\", axis=1)\n",
        "X = pd.concat([X, pd.get_dummies(['ocean_proximity'], prefix='ocean_proximity')], axis=1)\n",
        "X =  X.drop(columns=[\"ocean_proximity\"])\n",
        "X = X.fillna(X.mean())\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "X_train = X_train.to_numpy()\n",
        "X_dev = X_dev.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "y_dev = y_dev.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyCr_t-InC63",
        "outputId": "387f25a7-1d4a-4113-e1d6-b4916691fc17"
      },
      "source": [
        "print(\"Regression\")\n",
        "\n",
        "wrf = WRF(type='reg')\n",
        "rf_regression = RandomForestRegressor()\n",
        "# #Weighted Random Forest \n",
        "wrf_arguments = {\"n_trees\": [5, 7, 9],\n",
        "                 \"max_depth\": [3, 4, 5], \"n_features\": [1, 2], \"weight_type\": ['duv', 'sub']}\n",
        "# wrf_arguments = {\"n_trees\": [5],\n",
        "#                  \"max_depth\": [3], \"n_features\": [1], \"weight_type\": ['duv', 'sub']}\n",
        "wrf_best_params = tune(wrf, X_train, y_train, wrf_arguments, cv=5)\n",
        "print(\"\\nWRF best params are:\")\n",
        "print(wrf_best_params)\n",
        "\n",
        "#Random Forest \n",
        "gcv = GridSearchCV(rf_regression, {\"max_features\": [1, 2],\n",
        "                        \"n_estimators\": [5, 7, 9], \"max_depth\": [3, 4, 5]},scoring=\"neg_mean_squared_error\", cv=5, )\n",
        "gcv.fit(X_train, y_train)\n",
        "rfregression_best_params = gcv.best_params_\n",
        "print(\"RF best params are:\")\n",
        "print(rfregression_best_params)\n",
        "\n",
        "#Weighted Random Forest best params to test\n",
        "wrf_bestparams_ready = WRF(n_trees=wrf_best_params['n_trees'], max_depth=wrf_best_params['max_depth'],\n",
        "                n_features=wrf_best_params['n_features'],type='reg', weight_type=wrf_best_params['weight_type'])\n",
        "wrf_bestparams_ready.fit(X_train, y_train)\n",
        "\n",
        "#Random Forest best params to test\n",
        "rf_bestparams_ready = RandomForestClassifier(n_estimators=rfregression_best_params['n_estimators'],\n",
        "                                  max_depth=rfregression_best_params['max_depth'],\n",
        "                                  max_features=rfregression_best_params['max_features'])\n",
        "rf_bestparams_ready.fit(X_train, y_train)\n",
        "#Predict the models\n",
        "rf_prediction = rf_bestparams_ready.predict(X_dev)\n",
        "wrf_prediction = wrf_bestparams_ready.predict(X_dev)\n",
        "\n",
        "wrf_mean_square_error =  mean_squared_error(y_dev,wrf_prediction)\n",
        "rf_mean_square_error = mean_squared_error(y_dev, rf_prediction )\n",
        "\n",
        "\n",
        "print(\"WRF Mean Square Error:\")\n",
        "print(wrf_mean_square_error)\n",
        "\n",
        "print(\"RF Mean Square Error:\")\n",
        "print(rf_mean_square_error)\n",
        "\n",
        "\n",
        "# I calculated this as well to get values closer to 0-1\n",
        "wrf_mean_square_log_error =  mean_squared_log_error(y_dev,wrf_prediction)\n",
        "rf_mean_square_log_error = mean_squared_log_error(y_dev, rf_prediction )\n",
        "\n",
        "print(\"WRF Mean Square Log Error:\")\n",
        "print(wrf_mean_square_log_error)\n",
        "\n",
        "print(\"RF Mean Square Log Error:\")\n",
        "print(rf_mean_square_log_error)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regression\n",
            "\n",
            "WRF best params are:\n",
            "{'max_depth': 5, 'n_features': 2, 'n_trees': 9, 'weight_type': 'sub'}\n",
            "RF best params are:\n",
            "{'max_depth': 5, 'max_features': 2, 'n_estimators': 5}\n",
            "WRF Mean Square Error:\n",
            "6443374326.078873\n",
            "RF Mean Square Error:\n",
            "88321994918.59496\n",
            "WRF Mean Square Log Error:\n",
            "0.17194570275125007\n",
            "RF Mean Square Log Error:\n",
            "1.2309941144470435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AzHGnkOl_M4"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}